{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint\n",
    "import json\n",
    "import re\n",
    "import selenium\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d5689",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c87882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class webScraper:\n",
    "    \"\"\"\n",
    "    Scraping toolkit for Understat fixtures\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_url):\n",
    "        self.url = initial_url\n",
    "        self.driver = webdriver.Chrome()\n",
    "    \n",
    "    def start(self):\n",
    "        self.driver.get(self.url)\n",
    "    \n",
    "    def end(self):\n",
    "        self.driver.close()\n",
    "    \n",
    "    def get_match_numbers(self):\n",
    "        \"\"\"\n",
    "        Looks for any fixture IDs on any pages on Understat\n",
    "        \"\"\"\n",
    "        driver = self.driver\n",
    "        mi = driver.find_elements_by_class_name(\"match-info\")\n",
    "        fixture_nums = []\n",
    "        for i, x in enumerate(mi):\n",
    "            href = mi[i].get_attribute(\"href\")\n",
    "            fixture_nums.append(href[href.find(\"match/\")+6:])\n",
    "        return fixture_nums\n",
    "     \n",
    "    def get_all_match_numbers(self):\n",
    "        \"\"\"\n",
    "        Scrape all fixtures from season in question\n",
    "        \"\"\"\n",
    "        all_match_ids = []\n",
    "        for i in range(10000):\n",
    "            try:\n",
    "                all_match_ids = all_match_ids + get_match_numbers(self.driver)\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(6)\n",
    "            button = self.driver.find_element_by_class_name(\"calendar-prev\")\n",
    "            button.click()\n",
    "            print(len(all_match_ids))\n",
    "            time.sleep(6)\n",
    "            if len(all_match_ids)>400:\n",
    "                break\n",
    "        return all_match_ids\n",
    "    \n",
    "    def get_fixture_data(self, fix_id):\n",
    "        \"\"\"\n",
    "        Get all data for a particular fixture ID\n",
    "        \"\"\"\n",
    "        url = \"https://understat.com/match/\" + str(fix_id)\n",
    "        self.driver.get(url)\n",
    "        time.sleep(5)\n",
    "        match_info = dict()\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        \n",
    "        match_info[\"title_details\"] = str(soup.find_all(\"title\"))\n",
    "        \n",
    "        data1name = self.driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[4]/div/div[1]/div/label[1]\")\n",
    "        data1 = soup.find_all(\"table\")\n",
    "    \n",
    "        button = self.driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[4]/div/div[1]/div/label[2]\")\n",
    "        button.click()\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        data2 = soup.find_all(\"table\")\n",
    "    \n",
    "        data2name = button.text\n",
    "    \n",
    "        match_info[\"away_team_stats\"] = str(data2)\n",
    "        match_info[\"home_team_stats\"] = str(data1)\n",
    "        match_info[\"date\"] = title[title.find(\"(\")+1:title.find(\")\")]\n",
    "        match_info[\"home_team\"] = title[title.find(\">\")+1:title.find(\" \")]\n",
    "        match_info[\"away_team\"] = title[title.find(\"-\")+4:title.find(\"(\")-1]\n",
    "    \n",
    "        return match_info\n",
    "        \n",
    "    def get_all_fixture_data(self, list_of_ids):\n",
    "        return [self.get_fixture_data(x) for x in list_of_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "webs = webScraper(\"https://understat.com/league/EPL/2020\")\n",
    "webs.start()\n",
    "all_match_ids = webs.get_all_match_numbers()\n",
    "all_match_ids = list(set(all_match_ids)) # de-dupe\n",
    "fix_info = webs.get_all_fixture_data(all_match_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.dumps(fixture_info)\n",
    "with open('EPL_20_21a.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df4244",
   "metadata": {},
   "source": [
    "# Parsing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('EPL_20_21___old.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "df = json.loads(json_data)\n",
    "\n",
    "fixture_ids = list(df.keys())\n",
    "\n",
    "def fixture_data_ETL(fix_id):\n",
    "    \n",
    "    date = df[fix_id][\"date\"]\n",
    "    \n",
    "    home = pd.read_html(df[fix_id][\"home_team_stats\"])[0]\n",
    "    \n",
    "    try:\n",
    "        home[\"xG\"] = home[\"xG\"].apply(lambda x: float(x[0:4]))\n",
    "    except:\n",
    "        home[\"xG\"] = home[\"xG\"].apply(lambda x: float(x))\n",
    "    \n",
    "    try:\n",
    "        home[\"xA\"] = home[\"xA\"].apply(lambda x: float(x[0:4]))\n",
    "    except:\n",
    "        home[\"xA\"] = home[\"xA\"].apply(lambda x: float(x))\n",
    "    \n",
    "    home[\"team\"] = df[fix_id][\"home_team\"]\n",
    "    home[\"opposition\"] = df[fix_id][\"away_team\"]\n",
    "    home[\"home_or_away\"] = \"home\"\n",
    "    \n",
    "    home[\"team_Sh\"] = home.iloc[-1,:][\"Sh\"]\n",
    "    home[\"team_G\"] = home.iloc[-1,:][\"G\"]\n",
    "    home[\"team_KP\"] = home.iloc[-1,:][\"KP\"]\n",
    "    home[\"team_A\"] = home.iloc[-1,:][\"A\"]\n",
    "    home[\"team_xG\"] = home.iloc[-1,:][\"xG\"]\n",
    "    home[\"team_xA\"] = home.iloc[-1,:][\"xA\"]\n",
    "\n",
    "\n",
    "\n",
    "    away = pd.read_html(df[fix_id][\"away_team_stats\"])[0]\n",
    "    \n",
    "    try:\n",
    "        away[\"xG\"] = away[\"xG\"].apply(lambda x: float(x[0:4]))\n",
    "    except:\n",
    "        away[\"xG\"] = away[\"xG\"].apply(lambda x: float(x))\n",
    "    \n",
    "    try:\n",
    "        away[\"xA\"] = away[\"xA\"].apply(lambda x: float(x[0:4]))\n",
    "    except:\n",
    "        away[\"xA\"] = away[\"xA\"].apply(lambda x: float(x))\n",
    "    \n",
    "    away[\"team\"] = df[fix_id][\"away_team\"]\n",
    "    away[\"opposition\"] = df[fix_id][\"home_team\"]\n",
    "    away[\"home_or_away\"] = \"away\"\n",
    "    \n",
    "    away[\"team_Sh\"] = away.iloc[-1,:][\"Sh\"]\n",
    "    away[\"team_G\"] = away.iloc[-1,:][\"G\"]\n",
    "    away[\"team_KP\"] = away.iloc[-1,:][\"KP\"]\n",
    "    away[\"team_A\"] = away.iloc[-1,:][\"A\"]\n",
    "    away[\"team_xG\"] = away.iloc[-1,:][\"xG\"]\n",
    "    away[\"team_xA\"] = away.iloc[-1,:][\"xA\"]\n",
    "    \n",
    "    away = away.iloc[:-1,1:]\n",
    "    home = home.iloc[:-1,1:]\n",
    "    \n",
    "    home[\"date\"] = date\n",
    "    away[\"date\"] = date\n",
    "\n",
    "\n",
    "    away_team_stats = away[[\"team_Sh\", \"team_G\", \"team_KP\", \"team_A\", \"team_xG\", \"team_xA\"]].rename(columns={\n",
    "        \"team_Sh\":\"opponent_Sh\",\n",
    "        \"team_G\":\"opponent_G\",\n",
    "        \"team_KP\":\"opponent_KP\",\n",
    "        \"team_A\":\"opponent_A\",\n",
    "        \"team_xG\":\"opponent_xG\",\n",
    "        \"team_xA\":\"opponent_xA\",\n",
    "    }).iloc[0:11,:]\n",
    "    \n",
    "    home_team_stats = home[[\"team_Sh\", \"team_G\", \"team_KP\", \"team_A\", \"team_xG\", \"team_xA\"]].rename(columns={\n",
    "        \"team_Sh\":\"opponent_Sh\",\n",
    "        \"team_G\":\"opponent_G\",\n",
    "        \"team_KP\":\"opponent_KP\",\n",
    "        \"team_A\":\"opponent_A\",\n",
    "        \"team_xG\":\"opponent_xG\",\n",
    "        \"team_xA\":\"opponent_xA\",\n",
    "    }).iloc[0:11,:]\n",
    "    \n",
    "    home = pd.concat([home, away_team_stats], axis=1).ffill(axis=0)\n",
    "    away = pd.concat([away, home_team_stats], axis=1).ffill(axis=0)\n",
    "    \n",
    "    fixture_df = pd.concat([home,away]).reset_index().iloc[:,1:]\n",
    "    \n",
    "    fixture_df[\"title_details\"] = df[fix_id][\"title_details\"]\n",
    "    \n",
    "    return fixture_df\n",
    "\n",
    "\n",
    "big_df = fixture_data_ETL(fixture_ids[0])\n",
    "\n",
    "for i, n in enumerate(fixture_ids[1:]):#range(len(fixture_ids[1:])):\n",
    "    fix_id = fixture_ids[1:][i]\n",
    "    if i%25 == 0:\n",
    "        print(i)\n",
    "    big_df = pd.concat([big_df, fixture_data_ETL(fix_id)]).reset_index().iloc[:,1:]\n",
    "\n",
    "big_df[\"month\"] = big_df[\"date\"].apply(lambda x: x[:x.find(\" \")])\n",
    "\n",
    "big_df[\"day\"] = big_df[\"date\"].apply(lambda x: x[x.find(\" \"):][1:3])\n",
    "\n",
    "big_df[\"year\"] = big_df[\"date\"].apply(lambda x: x[x.find(\" \"):][4:])\n",
    "\n",
    "month_mapper = {\"January\":1,\"February\":2,\"March\":3,\"April\":4,\"May\":5,\"June\":6,\"July\":7,\"August\":8,\n",
    "               \"September\":9,\"October\":10,\"November\":11,\"December\":12}\n",
    "\n",
    "big_df[\"month\"] = big_df[\"month\"].map(month_mapper)\n",
    "\n",
    "big_df[\"datetime\"] = pd.to_datetime(big_df[[\"year\", \"month\",\"day\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6774260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_name_generator(title_instance, homeaway_instance):\n",
    "    titled = title_instance\n",
    "    dig_ind = re.search(r\"\\d\", titled).start()\n",
    "    name1 = titled[8:dig_ind-1]\n",
    "    first = re.search(r\"\\-\", titled).start()\n",
    "    second= re.search(r\"\\(\", titled).start()\n",
    "    name2 = titled[first+4:second-1]\n",
    "    \n",
    "    if homeaway_instance == \"home\":\n",
    "        team = name1\n",
    "        opponent = name2\n",
    "    else:\n",
    "        team = name2\n",
    "        opponent = name1\n",
    "    return (team, opponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672498f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = []\n",
    "opps = []\n",
    "for i,x in enumerate(big_df):\n",
    "    thingy = team_name_generator(big_df[\"title_details\"][i], big_df[\"home_or_away\"][i])\n",
    "    teams.append(thingy[0])\n",
    "    opps.append(thingy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb351077",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[\"team_fixed\"] = teams\n",
    "big_df[\"opposition_fixed\"] = opps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.to_csv(\"epl2021_new.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
